---
title: "Vignette - Units Data Preparation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(data.table)
library(measurements)
library(disperseR)
library(kableExtra)
```

In this vignette we show how we prepare the units data attached to this function. 

If you have not done so yet please create the project folders as follows. 

```{r}
create_dirs()
```

First, we need to download two files: Power Plant Emissions Data and 

Power [Plant Emissions Data](https://doi.org/10.7910/DVN/M3D2NR) and [Emission Data from The US Environmental Protection Agency](ftp://newftp.epa.gov/air/emismod/2014/v2/2014fd/emissions/). 

We start by downloading the first datasets. We will download it to the process folder, unzip it and read the csv: "2014fd_cb6_14j/inputs/ptegu/ptegu_2014NEIv2_POINT_20171103_final_21dec2017_nf_v2.csv"

### Download and read in data
```{r}
url <-"ftp://newftp.epa.gov/air/emismod/2014/v2/2014fd/emissions/2014fd_inputs_point.zip"
directory <- proc_dir
file <- file.path(directory, '2014fd_inputs_point.zip')
if (!file.exists(file)) {  # if file does not exist, download it 
  download.file(url = url, destfile = file)
}
unzip(file, exdir = directory) # unzip the file 
file <- file.path(directory, "2014fd_cb6_14j", "inputs", "ptegu", "ptegu_2014NEIv2_POINT_20171103_final_21dec2017_nf_v2.csv")
d_nei <- data.table::fread(file, skip = 18)
names(d_nei)
```

The other data set is hosted on "https://dataverse.harvard.edu/api/access/datafile/3086908?gbrecs=true". We specify this location and download it the same way. 

```{r}
url <-"https://dataverse.harvard.edu/api/access/datafile/3086908?gbrecs=true"
directory <- proc_dir
file <- file.path(directory, 'AMPD_Unit_with_Sulfur_Content_and_Regulations_with_Facility_Attributes.csv')
if (!file.exists(file)) { # if file does not exist, download it 
  download.file(url = url, destfile = file)
}
d_ampd <- data.table::fread(file)
names(d_ampd)
```

### Manipulation 

```{r}
d_nei_unique <- unique(d_nei[, .(
  facility_name,
  Facility.ID..ORISPL. = oris_facility_code,
  Unit.ID = oris_boiler_id,
  stkhgt,
  stkdiam,
  stktemp,
  stkvel,
  latitude,
  longitude
  )])
  
d_nei_unique <- d_nei_unique[Facility.ID..ORISPL. != "" & Unit.ID != ""]
d_nei_unique<- d_nei_unique[, Facility.ID..ORISPL.:=as.numeric(d_nei_unique$Facility.ID..ORISPL.)]
knitr::kable(head(d_nei_unique))
```

Here we show you how to prepare the 2005 set but you can choose other years. 

```{r}
d_ampd <- d_ampd[Year == 2005][, V1 := NULL]
file <- file.path(directory, "AMPD_2005.csv")
fwrite(d_ampd, file)

names(d_ampd)
```

```{r}
d_ampd_subset <- d_ampd[Fuel1.IsCoal == 1][, .(
  Facility.ID..ORISPL.,
  Unit.ID,
  Year,
  Month,
  Initial.Year.of.Operation,
  Sulfur.Content,
  Program.s. = Program.s..x,
  SO2.Phase = SO2.Phase.y,
  NOx.Phase = NOx.Phase.y,
  EPA.Region,
  NERC.Region,
  Source.Category = Source.Category.y,
  State = State.x,
  Facility.Latitude = Facility.Latitude.x,
  Facility.Longitude = Facility.Longitude.x,
  Has.SO2.Scrub,
  SO2..tons.,
  Has.NOx.Scrub,
  NOx..tons.,
  CO2..short.tons.,
  Heat.Input..MMBtu.,
  Gross.Load..MW.h.,
  Steam.Load..1000lb.,
  Max.Hourly.HI.Rate..MMBtu.hr.
  )]

names(d_ampd_subset)
```

```{r}
d_ampd_annual <- d_ampd_subset[, .(
  Facility.Latitude,
  Facility.Longitude,
  SO2..tons. = sum(SO2..tons., na.rm = TRUE),
  CO2..short.tons. = sum(CO2..short.tons., na.rm = TRUE),
  NOx..tons. = sum(NOx..tons., na.rm = TRUE)
  ),
  by = c("Facility.ID..ORISPL.", "Unit.ID", "Year")]


names(d_ampd_annual)
```

```{r}
d_ampd_annual <- unique(d_ampd_annual, by = c("Facility.ID..ORISPL.", "Unit.ID"))
d_ampd_annual<- d_ampd_annual[, Facility.ID..ORISPL.:=as.numeric(d_ampd_annual$Facility.ID..ORISPL.)]
knitr::kable(head(d_ampd_annual))
```

```{r}
M <- merge(d_ampd_annual, d_nei_unique, by = c("Facility.ID..ORISPL.", "Unit.ID"), all.x = TRUE)
file <- file.path(directory, 'merge_no_missing_info_coal_only_2005.csv')
fwrite(M, file)
```

```{r}
ampd <- M
ampd[, ID := paste(Facility.ID..ORISPL., Unit.ID, sep = "-")]
names(ampd)
```

```{r}
ampd <- ampd[, .(
  ID = ID,
  Latitude = Facility.Latitude,
  Longitude = Facility.Longitude,
  SOx = SO2..tons.,
  CO2 = CO2..short.tons.,
  NOx = NOx..tons.,
  Height = conv_unit(stkhgt, "ft", "m"),
  Diam = conv_unit(stkdiam, "ft", "m"),
  Velocity = conv_unit(stktemp, "ft_per_sec", "m_per_sec"),
  Temp = conv_unit(stkvel, "F", "K")
)]
```

Many of the units in the provided datasets do not have stack height data. In these cases, it is suggested in [Henneman et al. (2019)](https://www.sciencedirect.com/science/article/pii/S1352231019300731) to fill with the average stack height of all units. We do that and create a variable that flags if the height was inputed. 

```{r}
# create flag if Height is missing
ampd <- ampd[, inputed:=0]
ampd$inputed[is.na(ampd$Height)]<-1
# impute
ampd$Height[is.na(ampd$Height)]<-mean(ampd$Height, na.rm=T)
```

There are duplicates from the NEI datbase, where for the same stack  (= same facility + same unit) may be reported several times for the same pollutant with different stack info

```{r}
ampd <- unique(ampd, by = "ID")
file <- file.path(directory, "final_merge_nei_ampd_all_units_2005.csv")
write.csv(data.frame(ampd), file)
```

The ampd file present in your environment should be now ready to use. It was attached to the `disperser` package as `units2005`.  

