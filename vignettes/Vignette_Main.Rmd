---
title: "Vignette - Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this vignette you will see how to use the `disperser` package. We will walk you through an example step by step. In this vignette we are using functions that will automatically download the needed data for you to run your analysis for the United States. We also provide other vignettes that show you how we processed the data used in case you want to run the analysis for a different country or using different data sources. 

## The step up 

We start by importing the packages that we will use in the analysis. Make sure you have them installed. The functions in `disperser` are based on the `data.table` package for faster computation, but this vignette contains many functions from the `tidyverse` family to illustrate another syntax. 

```{r, message = FALSE, warning = FALSE}
library(disperseR) # our package
library(ncdf4)
library(data.table)
library(tidyverse)
library(kableExtra)
library(parallel)
library(sf)
library(viridis)
library(ggplot2)
library(scales)
library(maps)
library(ggsn)
```

The `create_dirs` function creates a project folder with all the necessary subfolders that we need. Begin by setting up your project with this function. Provide the path to where you would like your project to live in the `location` argument. By default, this function will create a main directory on your desktop. It will also assign paths to string variables in your environment. 

The set up is the following:

* `main`: The main folder where the project will be located. 
  + `input`: the input that we need for calculations. 
    * `zcta_500k`: ZCTA (A Zip Code Tabulation Area) shapefiles
    * `hpbl`: Monthly global planetary boundary layer files.
    * `meteo`: (reanalysis) meteorology files
  + `output`
    * `hysplit`: hyspdisp output (one file for each emissions event)
    * `ziplink`: files containing ZIP code linkages
    * `rdata`: RData files containing HyADS source-receptor matrices
    * `exp`: exposure per zipcode data 
  + `process`: temporary files that are created when the model is running and then deleted


```{r}
create_dirs()
```

## The inputs 

The next step is to get the data necessary for the analysis.  Let's start by loading data sets that are provided inside `disperser`.

### The crosswalk

ZIP code linkage procedure requires a ZCTA-to-ZIP code crosswalk file. ZCTAs are not exact geographic matches to ZIP codes, and multiple groups compile and maintain Crosswalk files. We used the Crosswalk mainted by UDS Mapper and preprocessed it also including information about the population size. While not necessary for the HYSPLIT model or processing of its outputs, population-weighted exposure metrics allow for direct comparisons between power plants. If you would like to know more details about how this crosswalk was prepared, we have attached a vignette that explains it. 

Let's load the crosswalk file. 

```{r}
crosswalk <- disperseR::crosswalk
crosswalk 
```

The disper package also includes monthly power plant emissions, load, and heat input data, accessible using the following command:

### The monthly powerplant emissions

```{r}
PP.units.monthly1995_2017 <- disperseR::PP.units.monthly1995_2017
PP.units.monthly1995_2017
```

### The shape files

Equally important is the ZCTA shapefile. Using the `get_data()` function and specifing the `data` argument to `"zctashapefile"` you can download and preprocess the file from the [US census website](http://www2.census.gov/geo/tiger/GENZ2017/shp/cb_2017_us_zcta510_500k.zip) automatically. If the file already exists in the correct folder, this function will preprocess it and load it into your R environment. 

```{r}
zcta <- disperseR::get_data(data = "zctashapefile")
zcta
```

### Monthly mean planetary boundary layer heights 

Another input that we need are the monthly mean boundary layer heights. Again, using the `get_data()` function and specifing the `data` argument to `"pblheight"`. If you need more information about how this data has been preprocessed just check the special vignette that is available with the package. We used the data that you can find by clicking on this link [ESRL website](https://www.esrl.noaa.gov/psd/repository/entry/get/hpbl.mon.mean.nc?entryid=synth%3Ae570c8f9-ec09-4e89-93b4-babd5651e7a9%3AL05BUlIvTW9udGhsaWVzL21vbm9sZXZlbC9ocGJsLm1vbi5tZWFuLm5j). 

```{r}
pblheight <- disperseR::get_data(data = "pblheight")
pblheight
```

### The units data 

Now, we need to select the power plants that we will run. In this case, we'll use the two units in 2005 with the greatest SOx emissions. This package contains annual emissions and stack height data from [EPA's Air Markets Program Data](https://ampd.epa.gov/ampd/) and the [Energy Information Agency] (https://www.eia.gov/electricity/data/eia860/) for years 2003 to 2012 both included. If you would like to know how these data were prepared please see the special vignette that we have attached to this package. 


```{r}
unitsrun <- disperseR::units2005 %>% 
  dplyr::top_n(2, SOx) # get the two most polluting plants

# transform to data table 
unitsrun <- data.table::data.table(unitsrun) 
knitr::kable(head(unitsrun))
```

### The meteorology files

Download reanalysis meteorology files.

Please download the files using the code below before the *first* run of `run_fac_parallel`. This package does not provide a function that would download the files in parallel because the parallel code does not handle the download well split over multiple cores. Below is shown code to test for the three meteorology files needed for the present run, and download them if they are not already in the `meteo_dir`. The reanalysis met files are about 120 MB each.

If you, for example, want to donwload files for Jan-Mar 2005, you just have to use the `get_data()` function specifying the `data` argument to `metfiles`, `start.year` to `"2005"`, `start.month` to `"01"`, `end.year` to `"2005"`, and `end.month` to `"03"`. See below.

```{r}
disperseR::get_data(data="metfiles", start.year="2005", start.month="01", end.year="2005", end.month="3")
```

Now the data should be downloaded to your `meteo_dir` directory and we can start the analysis.

## Analysis

### Define time periods to be run

HYSPLIT, as applied here, tracks air parcels emitted at certain times and locations. `hyspdisp` refers to these as *emissions events*. Once HYSPLIT has been run for each emissions event, the simulated parcel locations are aggregated by source, time, and location. The functions below are written to enable runs of many emissions events.

To define an object that includes all emission events in a given time period, we can use the helper function `define_inputtimes`. This takes as inputs a starting and ending day, and outputs a table of value whose rows will later correspond to inputs into the main `hyspdisp` worker functions. The following command combines the units defined above with four times a day for January-March in 2005 (we show an example for this short period of time here to keep the computation manageable for a desktop computer). Four daily emissions events are defined by the `start_hours` variable, and `duration = 240` denotes that the emitted air parces are tracked for 240 hours (10 days). The resulting data.table the head of which is printed in the table below. 

```{r}
input_refs <- disperseR::define_inputs(units = unitsrun,
  startday = '2005-01-01',
  endday = '2005-03-30',
  start.hours =  c(0, 6, 12, 18),
  duration = 240)

knitr::kable(input_refs)
```

### Run HYSPLIT
The following examples show how to run a small subset (chosen to provide workable examples on a laptop) of emissions events, link to ZIP codes, and plot the results.

The `run_fac_parallel` function runs HYSPLIT for each emissions event. Inputs defined above, including the projected ZCTA shapefile (`zcta`), the ZIP-ZCTA crosswalk (`crosswalk`), the planetary boundary layer raster (`pblheight`), are necessary in the function call. With `link2zip = T` you can link dispersion patterns from individual emissions events to ZIP codes; this, however, somewhat violates the spirit of HyADS, since the large number of emissions events is used as a check on some other uncertainties introduced by the simplifying assumptions. 

**If you get an error running HYSPLIT, it is likely because the meteorology files did not download correctly. Check your `meteo_dir` to ensure all files are present and at least 100MB in size.**

Below we get the 10 indexes that will be used to sample the input data.

```{r}
run_sample <- round(seq(1, dim(input_refs)[1], length.out = 10)) 
```

So we will be using the following events. 

```{r}
knitr::kable(input_refs[run_sample])
```

Now we will run the fac model in parallel. The argument `X` should take the vector of indeces that we created above, the `run_sample` vector. 

`input.refs` should be the data table that is the result of the `define_input()` function. The `ztca` argument takes the `zcta` set by default, by you can change it. The `crosswalk` argument takes the `crosswalk` table by default, but you can change it.  

The `pbl.height` argument takes the `pblheight` set by default, but you can change it. 

The `detectCores()` function automatically detects the number of cores on your machine. We use it in the `mc.cores` of `mclapply`. If you would like to change on how many cores the model should be run please specify the number of cores. Here, for example we specified the number of cores to be the number of the cores on our machine minus 1. If you would like to run this model sequentially please set the number of cores to 1.

The package now has the possibility to use two types of species. The default one is `species = 'so2'`, but you can also use particulate sulfate `species = 'so4p'`. 

```{r}
hysp_raw <- parallel::mclapply(X = run_sample,
  FUN = run_fac_parallel,
  input.refs = input_refs,
  pbl.height = pblheight,
  ztca = ztca,
  species = 'so2',
  link2zip = F,
  proc_dir = proc_dir,
  overwrite = F,
  npart = 100,
  mc.cores = detectCores()-1)
```

